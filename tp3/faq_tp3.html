<!DOCTYPE html>
<html>
  <head>
    <nav class="navbar navbar-default">
      <div class="container-fluid">
	<div class="navbar-header">
	  <a class="navbar-brand" href="#">Gaétan Marceau Caron</a>
	</div>
	<div>
	  <ul class="nav navbar-nav">
	    <li><a href="index.html">Home</a></li>
	    <li class="active"><a href="course.html">Course</a></li>
	    <li><a href="project.html">Project</a></li>
	    <li><a href="about.html">About</a></li>
	  </ul>
	</div>
      </div>
    </nav>
    <title>Homepage of Gaetan Marceau Caron</title>
    <!-- <link rel="stylesheet" type="text/css" href="mystyle.css"> -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <!-- <link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.5/superhero/bootstrap.min.css" rel="stylesheet" integrity="sha256-obxCG8hWR3FEKvV19p/G6KgEYm1v/u1FnRTS7Bc4Ii8= sha512-8Xs5FuWgtHH1JXye8aQcxVEtLqozJQAu8nhPymuRqeEslT6nJ2OgEYeS3cKiXasOVxYPN80DDoOTKWgOafU1LQ==" crossorigin="anonymous"></head> -->
    <link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.5/superhero/bootstrap.min.css" rel="stylesheet">
  </head>
  <body>
    <div class="container">
      <h1>TP3: Informations complémentaires</h1>
      <ol>
		<li> ReLU est plus instable numériquement que sigmoid comme le montre la sortie suivante: 
		  <pre>➜ tp3 python miniNN.py --arch 512 256 128 64 32 16 --act relu --eta 0.1
			Description of the experiment
			----------
			Learning algorithm: Bprop
			Initial step-size: 0.1
			Network Architecture: [784 512 256 128  64  32  16  10]
			Number of parameters: 577178
			Minibatch size: 500
			Activation: relu
			----------
			epoch time(s) train_loss train_accuracy valid_loss valid_accuracy eta
			0 34.088973 1.05501324176 0.73442 1.02449834088 0.753 0.1
			1 72.013352 0.400611756873 0.88626 0.370395033188 0.8946 0.1
			2 109.391082 0.269314204469 0.92078 0.2539309312 0.926 0.1
			3 145.185271 0.200444249472 0.94152 0.194301336843 0.9431 0.1
			4 182.401165 0.159805589303 0.95382 0.16249501492 0.9516 0.1
			5 220.461121 0.132649499644 0.96138 0.143976164083 0.9577 0.1
			6 257.318606 0.113401429054 0.96698 0.132906239371 0.9619 0.1
			7 294.106988 0.0952553809142 0.97246 0.121991947873 0.9663 0.1
			8 331.375223 0.0812804018288 0.97648 0.114878873894 0.9677 0.1
			9 369.860622 0.069268213261 0.98036 0.109025969434 0.9689 0.1
			10 406.592976 0.0595599330522 0.98316 0.105422214014 0.9709 0.1
			11 433.037308 0.0508575228002 0.9857 0.102110415976 0.9716 0.1
			12 459.965189 0.0440024858212 0.9879 0.100909452774 0.9724 0.1
			13 486.197912 0.0377021830319 0.9898 0.0994866459805 0.9722 0.1
			14 516.736345 0.0334940561291 0.99108 0.099828195204 0.9721 0.1
			15 543.590287 0.0279807027791 0.99282 0.0978032208905 0.9728 0.1
			16 573.857672 0.0236601365023 0.99432 0.0974943004617 0.9738 0.1
			17 602.994027 0.0204261088632 0.99512 0.0976954848962 0.9742 0.1
			18 629.704406 0.018688582013 0.9953 0.100759060896 0.9743 0.1
			19 663.028797 1.3966662704 0.50168 1.3891392081 0.5053 0.1
		  </pre>
		  Pour corriger ce problème, vous pouvez réduire le step-size, ce qui réduit aussi la vitesse d'entraînement.
		  Vous pouvez aussi utiliser une règle adaptative qui diminue le step-size si le coût augmente à l'epoch suivant.
		  Voici une règle adaptative qui multiplie le step-size par 1.2 si le coût diminue et multiplie par 0.5 si le coût augmente.
		  Si le coût augmente, on rétablit les paramètres de l'epoch précédent, ce qui permet d'éviter les erreurs à l'epoch 21 et 26.
		  <pre>
			➜ tp3 python miniNN.py --arch 512 256 128 64 32 16 --act relu --eta 0.1
			Description of the experiment
			----------
			Learning algorithm: Bprop
			Initial step-size: 0.1
			Network Architecture: [784 512 256 128  64  32  16  10]
			Number of parameters: 577178
			Minibatch size: 500
			Activation: relu
			----------
			epoch time(s) train_loss train_accuracy valid_loss valid_accuracy eta
			0 31.776705 1.05501324176 0.73442 1.02449834088 0.753 0.12
			1 68.745913 0.377486167973 0.89248 0.346343627336 0.9013 0.144
			2 105.550823 0.237564293915 0.93132 0.223548344204 0.935 0.1728
			3 140.377148 0.170259718411 0.94996 0.168735151816 0.9513 0.20736
			4 178.504246 0.136935430075 0.95906 0.145889496403 0.9587 0.248832
			5 215.937616 0.115630282646 0.96454 0.131182844635 0.9617 0.2985984
			6 251.613644 0.110839228744 0.96698 0.130240799271 0.9647 0.35831808
			7 286.664195 0.10233502007 0.96964 0.126715576215 0.9656 0.429981696
			8 321.156201 1.76823146523 0.31222 1.79885014122 0.3647 0.214990848
			9 361.738225 0.062733893194 0.98258 0.10004399253 0.9711 0.2579890176
			10 399.89704 0.0469564973907 0.98708 0.0941429616574 0.974 0.30958682112
			11 437.312741 0.0360219663429 0.99002 0.0900327774078 0.9763 0.371504185344
			12 473.088099 1.64017222475 0.3788 1.56183298906 0.4104 0.185752092672
			13 509.534037 0.0275554240265 0.99252 0.0894495818038 0.9774 0.222902511206
			14 546.972181 0.02216964883 0.99422 0.0896593470682 0.9775 0.267483013448
			15 580.400894 0.0170802672155 0.99558 0.0888771864043 0.979 0.320979616137
			16 610.518712 2.15617672063 0.16294 2.09808615088 0.2713 0.160489808069
			17 646.249303 0.0122748650744 0.99734 0.0880553266821 0.9793 0.192587769682
			18 682.20708 0.00996026181511 0.99772 0.0893522878908 0.9793 0.231105323619
			19 715.681066 0.00810873572401 0.99808 0.0922847472543 0.9794 0.277326388343
			20 749.057029 0.00695776841869 0.9983 0.0957431653003 0.9796 0.332791666011
			21 783.837513 nan 0.09864 nan 0.0991 0.166395833006
			22 819.620186 0.00442040435738 0.99922 0.0949815178699 0.9796 0.199674999607
			23 855.762985 0.00351298568753 0.99952 0.0966591820165 0.9798 0.239609999528
			24 890.647647 0.00283917424057 0.99966 0.100158213228 0.9798 0.287531999434
			25 925.765545 0.00223688456685 0.99974 0.10237602952 0.9799 0.34503839932
			26 961.604879 nan 0.09864 nan 0.0991 0.17251919966
			27 997.830004 0.00144975924598 0.99986 0.103756940823 0.98 0.207023039592
			28 1033.303451 0.00120823357769 0.99992 0.105930555643 0.9798 0.248427647511
			29 1069.352003 0.00102451314933 0.99994 0.108407104479 0.9797 0.298113177013
			30 1106.08137 0.000855783285687 0.99994 0.110778801278 0.9798 0.357735812415
			31 1140.933635 0.000698096652708 1.0 0.113356690818 0.9799 0.429282974898
			32 1176.488849 0.000585771513978 0.99998 0.115907076273 0.9801 0.515139569878
		  </pre>
		</li>
    </div>
  </body>
</html>
